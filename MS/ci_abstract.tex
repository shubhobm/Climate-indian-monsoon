\documentclass[11pt,twocolumn,twoside]{IEEEtran}
\usepackage{amsmath}
\usepackage[pdftex]{epsfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{mycommands,url,cite,hyperref,caption,subfigure}
\include{graphicsx}

\pagestyle{fancy}
%\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\rhead{\includegraphics[height=0.6in]{CITransparentwithTitleForWeb.png}}
\fancyhead[LO]{\sc Selecting Predictors of Indian Monsoon} % shorter form of title to fit in space
\fancyhead[LE]{\sc Majumdar, Dietz, Chatterjee} % author list or et al., to fit in space
\chead{}
\cfoot{}

\begin{document}
\title{\vspace{0.2in}\sc Identifying Driving Factors Behind Indian Monsoon Precipitation using Model Selection based on Data Depth}
\author{Subhabrata Majumdar$^1$\thanks{Corresponding author: S. Majumdar, University of Minnesota Twin Cities, Minneapolis MN; majum010@umn.edu$^1$}, Lindsey Dietz$^1$, Snigdhansu Chatterjee$^1$}

\maketitle
\thispagestyle{fancy}

\begin{abstract}
We introduce a novel one-step model selection technique for general regression estimators, and implement it in a linear mixed model setup to identify important predictors affecting Indian Monsoon precipitation. Under very general assumptions, this technique correctly identifies the set of non-zero values in the true coefficient (of length $p$) by comparing only $p+1$ models. Here we use wild bootstrap to estimate the selection criterion. Mixed models built on predictors selected by our procedure are more stable and accurate than full models across testing years in predicting median daily rainfall at a station.
\end{abstract}

\section{Motivation}

Obtaining a meaningful statistical model of Indian summer monsoon precipitation is challenging from both physical and statistical perspective due to its erratic nature.  This is an extremely important problem because monsoon precipitation is the major source of water for the mostly seasonal agricultural practice in the subcontinent. Dietz and Chatterjee show in\cite{Dietz2014} and \cite{Dietz2015Chapter} that in addition to several covariates and climate variables (found in references cited therein) there is a need to include random effects in modeling to quantify different source of uncertainties, e.g. variation across years and weather stations.

Selecting important covariates is a challenging task in this problem since unlike traditional approaches like linear regression, our problem involves both fixed and random effects, and potentially heteroskedastic error structure. Also, linearity or other regression assumptions are not guaranteed to hold and are hard to verify within the current context. Consequently, traditional likelihood-based methods may suffer from lack of robustness, while ad hoc techniques like randomization imply strong hidden assumptions which are unlikely to hold with current data. Here we tackle all these issues by selecting covariates utilizing a general model selection criterion that depends on the behavior of coefficient estimates in the parameter space, and demonstrate efficacy of the resulting model in out-of-sample forecasts.

\section{Methods}
%\subsection{Robust PCA based on data depth}
%The depth of a point in multivariate space, say $\bfx \in \mathbb{R}^p$, is any scalar measure of its centrality with respect to a data cloud $\bfX$ (or equivalently the underlying distribution $F$)\cite{ZuoSerfling00c}, and is denoted by $D(\bfx, \bfX)$ (or $D(\bfx, F)$). Assuming that $F$ is elliptic, we can define multivariate rank vectors using data depth:

%\begin{equation}
%\tilde \bfx = \tilde D(\bfx, \bfX).\frac{\bfx - \bfmu}{\| \bfx - \bfmu \|}
%\end{equation}
%where $\tilde D(\bfx, \bfX) = \max_{\bfx \in \mathbb R^p} D(\bfx, \bfX) - D(\bfx, \bfX)$. The transformation from original data to these rank vectors keeps population eigenvectors constant, and doing PCA on the rank vectors yields robust estimates of these eigenvectors with fairly good efficiency \cite{Majumdar15}.

\subsection{Data depth-based model selection}
The depth of a point $\bfx \in \mathbb{R}^p$, is any scalar measure of its centrality with respect to a data cloud $\bfX$ (or equivalently the underlying distribution $F$\cite{ZuoSerfling00c}, and is denoted by $D(\bfx, \bfX)$ (or $D(\bfx, F)$). Consider now a regression setup where estimates of a coefficient vector $\bfbeta$, based on a sample of size $n$, follow sampling distributions that can be asymptotically approximated by elliptic distributions $F_n$ centered at $\bfbeta$ that approach unit mass at $\bfbeta$ as $n \rightarrow \infty$. In this context, we define a model selection criterion for any candidate model, specified by $\alpha$, the set of indices where $\bfbeta$ takes non-zero values:

\begin{equation}\label{eqn:CnEqn}
C_n(\alpha) = \mathbb E \left[ D \left( \tilde \bfbeta_\alpha, F_n \right) \right]
\end{equation}
Here $\tilde \bfbeta_\alpha$ is the estimate of $\bfbeta_\alpha$ obtained from data concatenated with 0 at indices not in $\alpha$, and $D$ is any depth function. When $\alpha$ does not contain all non-zero indices in the true model, we have $C_n(\alpha) \rightarrow 0$ as $n \rightarrow \infty$\cite{MajumdarMS}. Otherwise for any $n$, the criterion maximizes at the smallest correct model, say $\alpha_0$, and decreases monotonically as zero indices are added to $\alpha_0$ one-by-one. In a sample setup, the unknown distribution $F_n$ and the expectation in \ref{eqn:CnEqn} are estimated by bootstrap.

\begin{figure*}
\captionsetup{justification=centering, font=footnotesize}
\begin{center}
\subfigure[]{\epsfxsize=0.3\linewidth \epsfbox{rolling_predMSE_full_vs_reduced}}
\subfigure[]{\epsfxsize=0.3\linewidth \epsfbox{rolling_density2012_full_vs_reduced}}
\subfigure[]{\epsfxsize=0.3\linewidth \epsfbox{rolling_map2012_full_vs_reduced}}
\caption{(a) Comparison of MSE for full and reduced model predictions across years, (b) Density plot for actual log rainfall and predictions in year 2012, (c) Station-wise reduced model residuals for 2012}
\label{fig:prepost}
\end{center}
\end{figure*}

For large enough $n$, we can obtain the most parsimonious correct model from true $C_n$ values of only $p+1$ models where $p$ is the dimension of $\bfbeta$.  We use the following scheme:
\begin{enumerate}
\item Calculate $C_n$ for full model;
\item Drop a predictor, calculate $C_n$ for the reduced model;
\item Repeat for all $p$ predictors;
\item Collect predictors dropping which causes $C_n$ to decrease. These are the predictors in the smallest correct model.
\end{enumerate}

\subsection{Linear Mixed Models}
Linear mixed models add an extra layer of complexity above the standard linear model setup by assuming latent unobservable random effects. We define this model as:

\begin{equation}\label{eqn:mixedmodel}
\bfY = X \bfbeta + Z \bfgamma + \bfepsilon
\end{equation}
where $\bfY_{n\times 1}$ is the vector of responses, $X_{n \times p}$ is the matrix of predictors and $\bfbeta_{p\times 1}$ is the vector of coefficients, which are referred as \textit{fixed effects} here. The latent layer comes in the form of the \textit{random effect} vector $\bfgamma_{k\times 1}$ ($k < n$), and the random effects design matrix $Z_{n \times k}$. We assume that $\bfgamma \sim \mathcal{N}_k ({\bf 0}, \Sigma)$; $\Sigma_{k \times k}$ positive definite, and the random errors $\bfepsilon_{n \times 1} \sim N(0,\sigma^2)$.

\section{Data and implementation}

We use data from 36 weather stations across India for 1978-2012 to model daily median rainfall at a station within a year. In addition to station-specific variables of latitude, longitude, and elevation, we use yearly medians of local variables including maximum and minimum temperature, tropospheric temperature difference ($\Delta TT$), $u$- and $v$- winds at 200, 600 and 850 mb, Ni\~{n}o 3.4 anomaly and Indian Dipole Mode Index (DMI), as well as of global variables that have known connections with the Indian monsoon pattern.  These include 10 indices of the Madden-Julian Oscillation (MJO), 9 northern hemisphere teleconnection indices, solar flux levels, and land-ocean Temperature Anomaly (TA).

We implement the model in \ref{eqn:mixedmodel} taking all the variables mentioned above as fixed effects, and year as a single random effect (i.e. $k=1$). We use separate wild bootstraps\cite{Mammen93} on estimated random effects and residuals to obtain resampled observations. Among 35 predictors considered, 21 are selected by our procedure- all of which have been proposed in literature. TA seems to have a large influence.  We also note several MJO indices are selected when starting from a full model with everything but TA, but are dropped in favor of TA when it is included in the full model.

We use a 25 year rolling validation scheme to compare prediction performances of full and reduced models. For each of the years 2003--2012, we use the past 25 year's data as training data. Figure \ref{fig:prepost} summarizes some of the results. Predictions from the reduced model are generally more stable across testing years (less MSE in panel (a)) than those from full model. Also, as demonstrated by panels (b) and (c) for year 2012, the reduced model provides a less biased estimate of the true values.  We observe this reduction in bias for all 10 testing years.

Future work includes investigating spatio-temporal dependencies, detailed studies into algorithmic efficiency issues and further development of theoretical properties of the proposed model selection tool.

%\section*{Acknowledgments}
%This work is partially supported by the NSF grant IIS-1029711.

\bibliographystyle{ieeetr}
\bibliography{climate}

\end{document}